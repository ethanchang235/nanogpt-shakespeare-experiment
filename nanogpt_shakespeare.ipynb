{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMX6vNBBmAJUkmiiz01wl3x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ethanchang235/nanogpt-shakespeare-experiment/blob/main/nanogpt_shakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wyzZSD7-1BV",
        "outputId": "97dc173c-a231-4188-803f-8bf1850ae9ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 686, done.\u001b[K\n",
            "remote: Total 686 (delta 0), reused 0 (delta 0), pack-reused 686 (from 1)\u001b[K\n",
            "Receiving objects: 100% (686/686), 954.04 KiB | 19.88 MiB/s, done.\n",
            "Resolving deltas: 100% (387/387), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OboFyeRE_hfR",
        "outputId": "7f939f14-1217-4897-c655-b3ef34d224a1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nanoGPT  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd nanoGPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgZKJQoz_lKj",
        "outputId": "01feb3f3-92c1-48c9-c09d-13cc40edde20"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nanoGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQh7smBq_wtH",
        "outputId": "c693489c-f2f8-40bf-db8d-63f929ea6ac7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.3.0 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl (848.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m848.7/848.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 torch-2.6.0+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "cuda_available = torch.cuda.is_available()\n",
        "print(f\"CUDA available: {cuda_available}\")\n",
        "if cuda_available:\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"WARNING: CUDA not available, training will be on CPU and very slow!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhXDDCKOAw4F",
        "outputId": "1284a7b8-2130-4ea8-c87b-c56a2edc812e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu118\n",
            "CUDA available: True\n",
            "CUDA version: 11.8\n",
            "GPU name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python data/shakespeare_char/prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKztWbv5A7Zd",
        "outputId": "4c38e162-bb48-4341-faa5-0e719066c0d0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_shakespeare_char.py --device=cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-_m0M88A9aO",
        "outputId": "6c30d173-9fbc-4929-bf3f-615a65b0e8a9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: device = cuda\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 10.65M\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "W0527 04:33:20.774000 2586 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py:1948: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 264, in <module>\n",
            "    losses = estimate_loss()\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/train.py\", line 224, in estimate_loss\n",
            "    logits, loss = model(X, Y)\n",
            "                   ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\", line 574, in _fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/model.py\", line 181, in forward\n",
            "    x = block(x)\n",
            "        ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 1380, in __call__\n",
            "    return self._torchdynamo_orig_callable(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 1164, in __call__\n",
            "    result = self._inner_convert(\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 547, in __call__\n",
            "    return _compile(\n",
            "           ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 986, in _compile\n",
            "    guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 715, in compile_inner\n",
            "    return _compile_inner(code, one_graph, hooks, transform)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_utils_internal.py\", line 95, in wrapper_function\n",
            "    return function(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 750, in _compile_inner\n",
            "    out_code = transform_code_object(code, transform)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1361, in transform_code_object\n",
            "    transformations(instructions, code_options)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 231, in _fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 662, in transform\n",
            "    tracer.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2868, in run\n",
            "    super().run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\n",
            "    while self.step():\n",
            "          ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\n",
            "    self.dispatch_table[inst.opcode](self, inst)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3048, in RETURN_VALUE\n",
            "    self._return(inst)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3033, in _return\n",
            "    self.output.compile_subgraph(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/output_graph.py\", line 1101, in compile_subgraph\n",
            "    self.compile_and_call_fx_graph(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/output_graph.py\", line 1382, in compile_and_call_fx_graph\n",
            "    compiled_fn = self.call_user_compiler(gm)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/output_graph.py\", line 1432, in call_user_compiler\n",
            "    return self._call_user_compiler(gm)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/output_graph.py\", line 1483, in _call_user_compiler\n",
            "    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/output_graph.py\", line 1462, in _call_user_compiler\n",
            "    compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 130, in __call__\n",
            "    compiled_gm = compiler_fn(gm, example_inputs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 2340, in __call__\n",
            "    return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py\", line 1863, in compile_fx\n",
            "    return aot_autograd(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/backends/common.py\", line 83, in __call__\n",
            "    cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/aot_autograd.py\", line 1155, in aot_module_simplified\n",
            "    compiled_fn = dispatch_and_compile()\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/aot_autograd.py\", line 1131, in dispatch_and_compile\n",
            "    compiled_fn, _ = create_aot_dispatcher_function(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/aot_autograd.py\", line 580, in create_aot_dispatcher_function\n",
            "    return _create_aot_dispatcher_function(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/aot_autograd.py\", line 830, in _create_aot_dispatcher_function\n",
            "    compiled_fn, fw_metadata = compiler_fn(\n",
            "                               ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 203, in aot_dispatch_base\n",
            "    compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/aot_autograd.py\", line 489, in __call__\n",
            "    return self.compiler_fn(gm, example_inputs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py\", line 1741, in fw_compiler_base\n",
            "    return inner_compile(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py\", line 569, in compile_fx_inner\n",
            "    return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/repro/after_aot.py\", line 102, in debug_wrapper\n",
            "    inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py\", line 685, in _compile_fx_inner\n",
            "    mb_compiled_graph = fx_codegen_and_compile(\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py\", line 1129, in fx_codegen_and_compile\n",
            "    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py\", line 1044, in codegen_and_compile\n",
            "    compiled_fn = graph.compile_to_module().call\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/graph.py\", line 2027, in compile_to_module\n",
            "    return self._compile_to_module()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/graph.py\", line 2068, in _compile_to_module\n",
            "    mod = PyCodeCache.load_by_key_path(\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/codecache.py\", line 2759, in load_by_key_path\n",
            "    mod = _reload_python_module(key, path)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 45, in _reload_python_module\n",
            "    exec(code, mod.__dict__, mod.__dict__)\n",
            "  File \"/tmp/torchinductor_root/5m/c5mfbdzyxtpzoeuchwt4smwyeleyb66rq4sa3zm7e7c5hz2gkd7q.py\", line 649, in <module>\n",
            "    async_compile.wait(globals())\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/async_compile.py\", line 305, in wait\n",
            "    scope[key] = result.result()\n",
            "                 ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/codecache.py\", line 3241, in result\n",
            "    result = self.future.result()\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n",
            "    return self.__get_result()\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "SubprocException: An exception occurred in a subprocess:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/triton/backends/nvidia/compiler.py\", line 356, in make_cubin\n",
            "    subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 571, in run\n",
            "    raise CalledProcessError(retcode, process.args,\n",
            "subprocess.CalledProcessError: Command '['/usr/local/lib/python3.11/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_75', '/tmp/tmp6653k6pd.ptx', '-o', '/tmp/tmp6653k6pd.ptx.o']' returned non-zero exit status 255.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 279, in do_job\n",
            "    result = job()\n",
            "             ^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 68, in _worker_compile_triton\n",
            "    load_kernel().precompile(warm_cache_only=True)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 293, in precompile\n",
            "    compiled_binary, launcher = self._precompile_config(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 493, in _precompile_config\n",
            "    binary = triton.compile(*compile_args, **compile_kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/triton/compiler/compiler.py\", line 279, in compile\n",
            "    next_module = compile_ir(module, metadata)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/triton/backends/nvidia/compiler.py\", line 389, in <lambda>\n",
            "    stages[\"cubin\"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.capability)\n",
            "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/triton/backends/nvidia/compiler.py\", line 374, in make_cubin\n",
            "    raise RuntimeError(f'{error}\\n'\n",
            "RuntimeError: Internal Triton PTX codegen error\n",
            "`ptxas` stderr:\n",
            "ptxas /tmp/tmp6653k6pd.ptx, line 67; error   : Feature '.bf16' requires .target sm_80 or higher\n",
            "ptxas /tmp/tmp6653k6pd.ptx, line 67; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher\n",
            "ptxas /tmp/tmp6653k6pd.ptx, line 70; error   : Feature '.bf16' requires .target sm_80 or higher\n",
            "ptxas /tmp/tmp6653k6pd.ptx, line 70; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher\n",
            "ptxas /tmp/tmp6653k6pd.ptx, line 73; error   : Feature '.bf16' requires .target sm_80 or higher\n",
            "ptxas /tmp/tmp6653k6pd.ptx, line 73; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher\n",
            "ptxas /tmp/tmp6653k6pd.ptx, line 76; error   : Feature '.bf16' requires .target sm_80 or higher\n",
            "ptxas /tmp/tmp6653k6pd.ptx, line 76; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher\n",
            "ptxas /tmp/tmp6653k6pd.ptx, line 79; error   : Feature '.bf16' requires .target sm_80 or higher\n",
            "ptxas /tmp/tmp6653k6pd.ptx, line 79; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher\n",
            "ptxas /tmp/tmp6653k6pd.ptx, line 82; error   : Feature '.bf16' requires .target sm_80 or higher\n",
            "ptxas /tmp/tmp6653k6pd.ptx, line 82; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher\n",
            "ptxas /tmp/tmp6653k6pd.ptx, line 85; error   : Feature '.bf16' requires .target sm_80 or higher\n",
            "ptxas /tmp/tmp6653k6pd.ptx, line 85; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher\n",
            "ptxas /tmp/tmp6653k6pd.ptx, line 88; error   : Feature '.bf16' requires .target sm_80 or higher\n",
            "ptxas /tmp/tmp6653k6pd.ptx, line 88; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher\n",
            "ptxas fatal   : Ptx assembly aborted due to errors\n",
            "\n",
            "Repro command: /usr/local/lib/python3.11/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_75 /tmp/tmp6653k6pd.ptx -o /tmp/tmp6653k6pd.ptx.o\n",
            "\n",
            "\n",
            "\n",
            "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "\n",
            "\n",
            "You can suppress this exception and fall back to eager by setting:\n",
            "    import torch._dynamo\n",
            "    torch._dynamo.config.suppress_errors = True\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_shakespeare_char.py --device=cuda --compile=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iyuBs8vBjDn",
        "outputId": "f4409fe2-145a-4ce6-9a90-46abf8d586a5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: device = cuda\n",
            "Overriding: compile = False\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 10.65M\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.2874, val loss 4.2823\n",
            "iter 0: loss 4.2664, time 68959.34ms, mfu -100.00%\n",
            "iter 10: loss 3.1420, time 506.88ms, mfu 0.74%\n",
            "iter 20: loss 2.7344, time 505.65ms, mfu 0.74%\n",
            "iter 30: loss 2.6177, time 507.52ms, mfu 0.74%\n",
            "iter 40: loss 2.5734, time 510.15ms, mfu 0.73%\n",
            "iter 50: loss 2.5265, time 513.88ms, mfu 0.73%\n",
            "iter 60: loss 2.5109, time 516.71ms, mfu 0.73%\n",
            "iter 70: loss 2.4934, time 517.95ms, mfu 0.73%\n",
            "iter 80: loss 2.4954, time 521.54ms, mfu 0.73%\n",
            "iter 90: loss 2.4687, time 523.38ms, mfu 0.73%\n",
            "iter 100: loss 2.4546, time 525.61ms, mfu 0.73%\n",
            "iter 110: loss 2.4568, time 530.53ms, mfu 0.72%\n",
            "iter 120: loss 2.4253, time 530.12ms, mfu 0.72%\n",
            "iter 130: loss 2.4128, time 528.89ms, mfu 0.72%\n",
            "iter 140: loss 2.3956, time 527.72ms, mfu 0.72%\n",
            "iter 150: loss 2.4113, time 524.18ms, mfu 0.72%\n",
            "iter 160: loss 2.3737, time 523.78ms, mfu 0.72%\n",
            "iter 170: loss 2.3457, time 522.33ms, mfu 0.72%\n",
            "iter 180: loss 2.3300, time 522.63ms, mfu 0.72%\n",
            "iter 190: loss 2.2454, time 520.03ms, mfu 0.72%\n",
            "iter 200: loss 2.2050, time 520.38ms, mfu 0.72%\n",
            "iter 210: loss 2.1368, time 521.61ms, mfu 0.72%\n",
            "iter 220: loss 2.1348, time 521.48ms, mfu 0.72%\n",
            "iter 230: loss 2.0737, time 521.32ms, mfu 0.72%\n",
            "iter 240: loss 2.0827, time 520.56ms, mfu 0.72%\n",
            "step 250: train loss 1.9556, val loss 2.0640\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 2.0305, time 73992.63ms, mfu 0.64%\n",
            "iter 260: loss 1.9761, time 519.91ms, mfu 0.65%\n",
            "iter 270: loss 1.9791, time 520.78ms, mfu 0.66%\n",
            "iter 280: loss 1.9667, time 521.88ms, mfu 0.66%\n",
            "iter 290: loss 1.9120, time 521.74ms, mfu 0.67%\n",
            "iter 300: loss 1.9046, time 523.09ms, mfu 0.67%\n",
            "iter 310: loss 1.8587, time 524.11ms, mfu 0.68%\n",
            "iter 320: loss 1.8496, time 520.56ms, mfu 0.68%\n",
            "iter 330: loss 1.8102, time 519.55ms, mfu 0.68%\n",
            "iter 340: loss 1.7844, time 520.96ms, mfu 0.69%\n",
            "iter 350: loss 1.8230, time 523.15ms, mfu 0.69%\n",
            "iter 360: loss 1.7654, time 522.30ms, mfu 0.69%\n",
            "iter 370: loss 1.7401, time 523.58ms, mfu 0.69%\n",
            "iter 380: loss 1.7260, time 523.49ms, mfu 0.70%\n",
            "iter 390: loss 1.7341, time 521.64ms, mfu 0.70%\n",
            "iter 400: loss 1.7549, time 521.77ms, mfu 0.70%\n",
            "iter 410: loss 1.6939, time 523.85ms, mfu 0.70%\n",
            "iter 420: loss 1.7189, time 524.03ms, mfu 0.70%\n",
            "iter 430: loss 1.6854, time 524.03ms, mfu 0.70%\n",
            "iter 440: loss 1.6482, time 523.67ms, mfu 0.70%\n",
            "iter 450: loss 1.6479, time 521.84ms, mfu 0.70%\n",
            "iter 460: loss 1.5926, time 523.84ms, mfu 0.71%\n",
            "iter 470: loss 1.6564, time 523.04ms, mfu 0.71%\n",
            "iter 480: loss 1.6190, time 524.26ms, mfu 0.71%\n",
            "iter 490: loss 1.5902, time 522.97ms, mfu 0.71%\n",
            "step 500: train loss 1.5222, val loss 1.7280\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 1.5996, time 73948.92ms, mfu 0.64%\n",
            "iter 510: loss 1.6085, time 521.74ms, mfu 0.64%\n",
            "iter 520: loss 1.5892, time 522.29ms, mfu 0.65%\n",
            "iter 530: loss 1.5629, time 522.26ms, mfu 0.66%\n",
            "iter 540: loss 1.6236, time 524.49ms, mfu 0.66%\n",
            "iter 550: loss 1.5668, time 524.32ms, mfu 0.67%\n",
            "iter 560: loss 1.5635, time 523.46ms, mfu 0.67%\n",
            "iter 570: loss 1.5650, time 523.70ms, mfu 0.68%\n",
            "iter 580: loss 1.5366, time 523.65ms, mfu 0.68%\n",
            "iter 590: loss 1.5033, time 523.00ms, mfu 0.68%\n",
            "iter 600: loss 1.5239, time 525.02ms, mfu 0.69%\n",
            "iter 610: loss 1.5384, time 522.97ms, mfu 0.69%\n",
            "iter 620: loss 1.5243, time 522.41ms, mfu 0.69%\n",
            "iter 630: loss 1.5065, time 523.84ms, mfu 0.69%\n",
            "iter 640: loss 1.4679, time 521.67ms, mfu 0.69%\n",
            "iter 650: loss 1.5063, time 521.68ms, mfu 0.70%\n",
            "iter 660: loss 1.5036, time 523.45ms, mfu 0.70%\n",
            "iter 670: loss 1.4489, time 521.76ms, mfu 0.70%\n",
            "iter 680: loss 1.5118, time 522.85ms, mfu 0.70%\n",
            "iter 690: loss 1.4652, time 523.70ms, mfu 0.70%\n",
            "iter 700: loss 1.4932, time 522.02ms, mfu 0.70%\n",
            "iter 710: loss 1.4543, time 523.11ms, mfu 0.70%\n",
            "iter 720: loss 1.4346, time 521.34ms, mfu 0.71%\n",
            "iter 730: loss 1.4206, time 522.11ms, mfu 0.71%\n",
            "iter 740: loss 1.4269, time 524.26ms, mfu 0.71%\n",
            "step 750: train loss 1.3569, val loss 1.5843\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 750: loss 1.4244, time 73916.81ms, mfu 0.64%\n",
            "iter 760: loss 1.4434, time 521.86ms, mfu 0.64%\n",
            "iter 770: loss 1.4194, time 522.92ms, mfu 0.65%\n",
            "iter 780: loss 1.4270, time 522.94ms, mfu 0.66%\n",
            "iter 790: loss 1.4218, time 524.52ms, mfu 0.66%\n",
            "iter 800: loss 1.4315, time 524.71ms, mfu 0.67%\n",
            "iter 810: loss 1.4062, time 523.36ms, mfu 0.67%\n",
            "iter 820: loss 1.3947, time 523.49ms, mfu 0.68%\n",
            "iter 830: loss 1.3899, time 522.90ms, mfu 0.68%\n",
            "iter 840: loss 1.4037, time 522.29ms, mfu 0.68%\n",
            "iter 850: loss 1.3885, time 522.54ms, mfu 0.69%\n",
            "iter 860: loss 1.3894, time 524.02ms, mfu 0.69%\n",
            "iter 870: loss 1.3961, time 524.88ms, mfu 0.69%\n",
            "iter 880: loss 1.3748, time 523.08ms, mfu 0.69%\n",
            "iter 890: loss 1.3858, time 523.53ms, mfu 0.69%\n",
            "iter 900: loss 1.3637, time 523.96ms, mfu 0.70%\n",
            "iter 910: loss 1.3193, time 524.74ms, mfu 0.70%\n",
            "iter 920: loss 1.3589, time 522.89ms, mfu 0.70%\n",
            "iter 930: loss 1.3650, time 521.94ms, mfu 0.70%\n",
            "iter 940: loss 1.3446, time 527.23ms, mfu 0.70%\n",
            "iter 950: loss 1.3528, time 523.50ms, mfu 0.70%\n",
            "iter 960: loss 1.3682, time 525.82ms, mfu 0.70%\n",
            "iter 970: loss 1.3569, time 526.66ms, mfu 0.70%\n",
            "iter 980: loss 1.3544, time 524.53ms, mfu 0.70%\n",
            "iter 990: loss 1.3273, time 522.53ms, mfu 0.71%\n",
            "step 1000: train loss 1.2726, val loss 1.5187\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.3353, time 74148.24ms, mfu 0.64%\n",
            "iter 1010: loss 1.3386, time 520.79ms, mfu 0.64%\n",
            "iter 1020: loss 1.3106, time 522.71ms, mfu 0.65%\n",
            "iter 1030: loss 1.3343, time 522.58ms, mfu 0.66%\n",
            "iter 1040: loss 1.3615, time 523.59ms, mfu 0.66%\n",
            "iter 1050: loss 1.2950, time 521.88ms, mfu 0.67%\n",
            "iter 1060: loss 1.3412, time 520.31ms, mfu 0.67%\n",
            "iter 1070: loss 1.3334, time 525.15ms, mfu 0.68%\n",
            "iter 1080: loss 1.3430, time 521.56ms, mfu 0.68%\n",
            "iter 1090: loss 1.3579, time 522.55ms, mfu 0.68%\n",
            "iter 1100: loss 1.3164, time 522.24ms, mfu 0.69%\n",
            "iter 1110: loss 1.2998, time 523.02ms, mfu 0.69%\n",
            "iter 1120: loss 1.2955, time 523.75ms, mfu 0.69%\n",
            "iter 1130: loss 1.2962, time 522.88ms, mfu 0.69%\n",
            "iter 1140: loss 1.2945, time 522.64ms, mfu 0.70%\n",
            "iter 1150: loss 1.3081, time 523.80ms, mfu 0.70%\n",
            "iter 1160: loss 1.3320, time 521.62ms, mfu 0.70%\n",
            "iter 1170: loss 1.2986, time 521.27ms, mfu 0.70%\n",
            "iter 1180: loss 1.3187, time 523.31ms, mfu 0.70%\n",
            "iter 1190: loss 1.2687, time 522.25ms, mfu 0.70%\n",
            "iter 1200: loss 1.2932, time 522.19ms, mfu 0.70%\n",
            "iter 1210: loss 1.2684, time 523.62ms, mfu 0.70%\n",
            "iter 1220: loss 1.3107, time 521.67ms, mfu 0.71%\n",
            "iter 1230: loss 1.3020, time 521.78ms, mfu 0.71%\n",
            "iter 1240: loss 1.3017, time 523.88ms, mfu 0.71%\n",
            "step 1250: train loss 1.2009, val loss 1.4987\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1250: loss 1.2680, time 74036.49ms, mfu 0.64%\n",
            "iter 1260: loss 1.2811, time 518.96ms, mfu 0.64%\n",
            "iter 1270: loss 1.2658, time 524.49ms, mfu 0.65%\n",
            "iter 1280: loss 1.2563, time 523.85ms, mfu 0.66%\n",
            "iter 1290: loss 1.2909, time 524.11ms, mfu 0.66%\n",
            "iter 1300: loss 1.2974, time 522.59ms, mfu 0.67%\n",
            "iter 1310: loss 1.2359, time 523.65ms, mfu 0.67%\n",
            "iter 1320: loss 1.3019, time 521.59ms, mfu 0.68%\n",
            "iter 1330: loss 1.2632, time 521.73ms, mfu 0.68%\n",
            "iter 1340: loss 1.2957, time 521.69ms, mfu 0.68%\n",
            "iter 1350: loss 1.2526, time 522.54ms, mfu 0.69%\n",
            "iter 1360: loss 1.2760, time 522.84ms, mfu 0.69%\n",
            "iter 1370: loss 1.2521, time 524.85ms, mfu 0.69%\n",
            "iter 1380: loss 1.2582, time 522.91ms, mfu 0.69%\n",
            "iter 1390: loss 1.2401, time 522.29ms, mfu 0.70%\n",
            "iter 1400: loss 1.2541, time 521.85ms, mfu 0.70%\n",
            "iter 1410: loss 1.2472, time 522.19ms, mfu 0.70%\n",
            "iter 1420: loss 1.2676, time 522.07ms, mfu 0.70%\n",
            "iter 1430: loss 1.2386, time 520.90ms, mfu 0.70%\n",
            "iter 1440: loss 1.2458, time 523.06ms, mfu 0.70%\n",
            "iter 1450: loss 1.2323, time 523.40ms, mfu 0.70%\n",
            "iter 1460: loss 1.2360, time 522.92ms, mfu 0.70%\n",
            "iter 1470: loss 1.2206, time 521.09ms, mfu 0.71%\n",
            "iter 1480: loss 1.2121, time 521.11ms, mfu 0.71%\n",
            "iter 1490: loss 1.2326, time 522.33ms, mfu 0.71%\n",
            "step 1500: train loss 1.1520, val loss 1.4806\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1500: loss 1.1822, time 73757.76ms, mfu 0.64%\n",
            "iter 1510: loss 1.2292, time 518.70ms, mfu 0.65%\n",
            "iter 1520: loss 1.2225, time 521.70ms, mfu 0.65%\n",
            "iter 1530: loss 1.2588, time 521.35ms, mfu 0.66%\n",
            "iter 1540: loss 1.1907, time 522.23ms, mfu 0.66%\n",
            "iter 1550: loss 1.2270, time 523.64ms, mfu 0.67%\n",
            "iter 1560: loss 1.2067, time 524.37ms, mfu 0.67%\n",
            "iter 1570: loss 1.2332, time 522.64ms, mfu 0.68%\n",
            "iter 1580: loss 1.2110, time 521.72ms, mfu 0.68%\n",
            "iter 1590: loss 1.1923, time 520.33ms, mfu 0.68%\n",
            "iter 1600: loss 1.1984, time 524.39ms, mfu 0.69%\n",
            "iter 1610: loss 1.2360, time 523.57ms, mfu 0.69%\n",
            "iter 1620: loss 1.1811, time 523.79ms, mfu 0.69%\n",
            "iter 1630: loss 1.2087, time 524.11ms, mfu 0.69%\n",
            "iter 1640: loss 1.2037, time 520.34ms, mfu 0.70%\n",
            "iter 1650: loss 1.1814, time 523.82ms, mfu 0.70%\n",
            "iter 1660: loss 1.2112, time 523.45ms, mfu 0.70%\n",
            "iter 1670: loss 1.1972, time 522.62ms, mfu 0.70%\n",
            "iter 1680: loss 1.1986, time 524.29ms, mfu 0.70%\n",
            "iter 1690: loss 1.2027, time 523.40ms, mfu 0.70%\n",
            "iter 1700: loss 1.1852, time 524.64ms, mfu 0.70%\n",
            "iter 1710: loss 1.1779, time 524.42ms, mfu 0.70%\n",
            "iter 1720: loss 1.1769, time 521.53ms, mfu 0.70%\n",
            "iter 1730: loss 1.2018, time 523.63ms, mfu 0.71%\n",
            "iter 1740: loss 1.1711, time 522.65ms, mfu 0.71%\n",
            "step 1750: train loss 1.1025, val loss 1.4700\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1750: loss 1.1865, time 74034.86ms, mfu 0.64%\n",
            "iter 1760: loss 1.1901, time 520.81ms, mfu 0.64%\n",
            "iter 1770: loss 1.2014, time 525.82ms, mfu 0.65%\n",
            "iter 1780: loss 1.1981, time 523.76ms, mfu 0.66%\n",
            "iter 1790: loss 1.1889, time 525.54ms, mfu 0.66%\n",
            "iter 1800: loss 1.1887, time 523.91ms, mfu 0.67%\n",
            "iter 1810: loss 1.1613, time 522.35ms, mfu 0.67%\n",
            "iter 1820: loss 1.1672, time 522.46ms, mfu 0.68%\n",
            "iter 1830: loss 1.1741, time 523.33ms, mfu 0.68%\n",
            "iter 1840: loss 1.1618, time 522.61ms, mfu 0.68%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 300, in <module>\n",
            "    logits, loss = model(X, Y)\n",
            "                   ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/model.py\", line 181, in forward\n",
            "    x = block(x)\n",
            "        ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/model.py\", line 105, in forward\n",
            "    x = x + self.mlp(self.ln_2(x))\n",
            "            ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1915, in __getattr__\n",
            "    def __getattr__(self, name: str) -> Union[Tensor, \"Module\"]:\n",
            "\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py --out_dir=out-shakespeare-char --device=cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AQOqHmHHlhJ",
        "outputId": "82b8d60b-155b-47f3-bcd4-ca0180fce37a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-shakespeare-char\n",
            "Overriding: device = cuda\n",
            "number of parameters: 10.65M\n",
            "Loading meta from data/shakespeare_char/meta.pkl...\n",
            "\n",
            "\n",
            "ANGELO:\n",
            "And I will kiss your masters.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "My lord,\n",
            "My Lord Angelo hath been to lay a friar.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "No, now she was a man to straw;\n",
            "Now even we sent in the sea, to be satisfied\n",
            "By his eye soul, like a coldly he courtesy.\n",
            "\n",
            "ANGELO:\n",
            "Why you have sent for this?\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "If you have done evil to your doubt,\n",
            "Then here would so rize much a dangerous for the\n",
            "great shepherd. And here is Prince, Sir,\n",
            "What should be disadvantaged?\n",
            "\n",
            "ESCALUS:\n",
            "Go to keep and your true friar?\n",
            "\n",
            "AN\n",
            "---------------\n",
            "\n",
            "Men pardon me, you shall have hanged to myself:\n",
            "Sometimes me lady, sir, and in this last,\n",
            "In this score some stouth sound more spoils me.\n",
            "\n",
            "Servant:\n",
            "Peace, move, that are best.\n",
            "\n",
            "First Servingman:\n",
            "The six of Rome, sir, 'tis moved, the shoulder he\n",
            "deed out.\n",
            "\n",
            "First Servingman:\n",
            "This is a thousand our son's desires.\n",
            "\n",
            "Second Servingman:\n",
            "I have ta'en the voices of Capitol.\n",
            "\n",
            "First Servingman:\n",
            "My lord and no more anon whose parcel could tell me back.\n",
            "\n",
            "Third Servingman:\n",
            "And how thou art a forced of goodnes\n",
            "---------------\n",
            "\n",
            "Messenger.\n",
            "\n",
            "MARIANA:\n",
            "I cannot know.\n",
            "\n",
            "LUCIO:\n",
            "An't do you to another concupions, and\n",
            "to be resolved so.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "I will tell it word, and the rest\n",
            "of your honour. Camillo, call a better of it\n",
            "please you, to the report in twenty soldiers; a noble\n",
            "and discovery of Him.\n",
            "\n",
            "POLIXENES:\n",
            "I pray, my lord, woman;\n",
            "I hope and Master Francienna, Grey are his;\n",
            "That lost you, the duke and have been so much,\n",
            "Could teach not the bastards of my head,\n",
            "That smother been my face, but seeing at the argument,\n",
            "The\n",
            "---------------\n",
            "\n",
            "\n",
            "First Citizen:\n",
            "Ay, but if you go as; it be thought a very restore\n",
            "the enemy: but when here you were all prevailedges are\n",
            "patient, he would remain a risin. The peace I am a\n",
            "that becomes him, by any old goodly part a coulder,\n",
            "he shall be longed for a goodly prisoner, to amend to our\n",
            "gentleman, the say before a gentleman but to his power life a spoil. How how the\n",
            "cause will not be not would no more than wonser? manage than can thou hast\n",
            "a particular with his bed, which none more for her beggar, wh\n",
            "---------------\n",
            "\n",
            "That lambs like to this strange to see it before.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Tear me the king die recover'd up in these heart\n",
            "Into the flowers of the embrace\n",
            "Of my fortune imposition of the court.\n",
            "\n",
            "KING RICHARD III:\n",
            "But I will not stay and ne'er do love,\n",
            "As I would pursue the walls, with the stars, to my lamentation\n",
            "To bid my heart and my husband, by lists me with me,\n",
            "I commend the great blood of the soul whole I would,\n",
            "I should move be my soul's soul, it were dear her dead.\n",
            "\n",
            "DUKE OF YORK:\n",
            "My fortune is \n",
            "---------------\n",
            "\n",
            "\n",
            "MENENIUS:\n",
            "Having he done seen that, that this is a mile world\n",
            "In his corphing for the people, the other\n",
            "'dread shall be: if it be\n",
            "past fore a better word, though the first of good,\n",
            "And when then caside manners that set it our charity\n",
            "Will not redress. What is thine own is short, here is my hand?\n",
            "\n",
            "Second Servant:\n",
            "Not a little hand inheritable shall be received,\n",
            "prove in the senatory contradict of our brother;\n",
            "Nor by my shame and myself, that thought I may have do,\n",
            "And breathe sings for what is t\n",
            "---------------\n",
            "\n",
            "Shall be well his friends graft that she,\n",
            "And so say 'What, if my breast and thou wilt that I come:\n",
            "And, I know thy villain!\n",
            "\n",
            "MENENIUS:\n",
            "No, no more: but I am in all half.\n",
            "\n",
            "CORIOLANUS:\n",
            "This is not that Marcius in Rome, that must I will.\n",
            "\n",
            "First Citizen:\n",
            "Yes, so you are a dog, and so would hear it.\n",
            "\n",
            "Third Citizen:\n",
            "We have made it to see.\n",
            "\n",
            "First Citizen:\n",
            "We'll and lead water your great charity.\n",
            "\n",
            "MENENIUS:\n",
            "The fool, madam.\n",
            "\n",
            "CORIOLANUS:\n",
            "I hope put your true, and to xpeak.\n",
            "\n",
            "MENENIUS:\n",
            "Consul, good Marci\n",
            "---------------\n",
            "\n",
            "lay behold me; and not we'll save the same.\n",
            "\n",
            "Second Servingman:\n",
            "Faith, of the realm of the world, the waste\n",
            "thing in the power of the information.\n",
            "\n",
            "CLAUDIO:\n",
            "\n",
            "CLAUDIO:\n",
            "I know not, good Christians.\n",
            "\n",
            "Shepherd:\n",
            "Nay, buy the way, and to see her cold strange in her child.\n",
            "\n",
            "ESCALUS:\n",
            "Your honest care; but now, sir; this is that your country.\n",
            "\n",
            "ISABELLA:\n",
            "It is no more wistern of this goodness heart.\n",
            "\n",
            "ANGELO:\n",
            "To me for her two words in the business.\n",
            "\n",
            "CAMILLO:\n",
            "So much a son, sir, why we are as in the world?\n",
            "---------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/sample.py\", line 87, in <module>\n",
            "    y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/model.py\", line 316, in generate\n",
            "    logits, _ = self(idx_cond)\n",
            "                ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/model.py\", line 181, in forward\n",
            "    x = block(x)\n",
            "        ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/model.py\", line 104, in forward\n",
            "    x = x + self.attn(self.ln_1(x))\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/model.py\", line 64, in forward\n",
            "    y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}